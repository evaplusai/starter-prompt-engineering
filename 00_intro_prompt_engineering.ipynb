{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Starter Prompt Engineering\n",
        "\n",
        "1. Getting started - setup env variables\n",
        "2. Using OpenAI library - client, roles\n",
        "3. Prompting examples - few shot, chain of thought\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note: How to get OpenAI key:\n",
        "\n",
        "Create an account with OpenAI [here](https://platform.openai.com/signup) if you do not have one.\n",
        "\n",
        "Click on the \"Settings\" icon at the top right,  then on the left menu navigate to \"API keys\". Click on \" Create new secret key\" and complete the screen. Make sure you Copy the key( you can always generate new one).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S4YFYTyPs5cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "First load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ],
      "metadata": {
        "id": "BD72fqZA72CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dependencies OpenAI library\n",
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "cO2w4bs78Lx7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Environment Variables"
      ],
      "metadata": {
        "id": "eakszIMU8SED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import openai"
      ],
      "metadata": {
        "id": "nhxAWZTj5XB0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
      ],
      "metadata": {
        "id": "Iu9b-aF08dUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f1459f-dd64-46a4-9e74-e91d51f45056"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. First Prompt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kWDnM32m8vy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use the ChatCompletion create method to interact with the \"gpt-4.1-nano\" model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "\n",
        "\n",
        "*   developer\n",
        "*   assistant\n",
        "*   user\n",
        "\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n"
      ],
      "metadata": {
        "id": "YL0HCv88Ay7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll explore these roles in more depth - but for now just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Again we will use latest model\n",
        "\n",
        "We'll use the `gpt-4.1` or `gpt-4.1-nano` model as stated above.\n",
        "\n",
        "Let's look at an example!\n"
      ],
      "metadata": {
        "id": "6elE8d_MBDRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models.\n",
        "\n",
        "> NOTE: You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) whenever you get stuck, have questions, or want to dive deeper."
      ],
      "metadata": {
        "id": "JWobf88a6UxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "wH0xIjtY6dum"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and CrewAI?\"\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrBeC-jZ5x40",
        "outputId": "e6581bf6-b83b-44d6-ec53-c9eefb086248"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BUm7al7FD1qTYZGZSpItNMHiWgtmg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and CrewAI are both tools related to artificial intelligence and natural language processing, but they serve different purposes and have distinct features. Here's an overview of their differences:\\n\\n**LangChain**\\n\\n- **Purpose:** A framework designed for building applications that incorporate large language models (LLMs). It simplifies the development of complex AI-powered workflows such as chatbots, retrieval-augmented generation systems, and general language understanding tasks.\\n\\n- **Core Features:**\\n  - Modular components for chaining together prompts, models, and data sources.\\n  - Support for memory, state management, and complex logic.\\n  - Integration with various LLM providers (OpenAI, Cohere, etc.).\\n  - Tools for document retrieval, question answering, and augmenting LLMs with external knowledge.\\n\\n- **Use Cases:** Building AI assistants, chatbots, knowledge bases, and other applications requiring sophisticated prompt management and multi-step pipelines.\\n\\n---\\n\\n**CrewAI**\\n\\n- **Purpose:** A platform focused on facilitating collaborative AI workflows, often emphasizing team-based AI development, prompt management, and task coordination among multiple users.\\n\\n- **Core Features:**\\n  - Team collaboration tools for managing prompts, models, and AI tasks.\\n  - Workflow organization and task assignment.\\n  - Interface for managing AI projects with multiple contributors.\\n  - Emphasis on enterprise or team-level AI application development and management.\\n\\n- **Use Cases:** Teams developing and managing AI applications collaboratively, enterprise AI deployment, prompt governance, and shared AI tool management.\\n\\n---\\n\\n**Summary of Key Differences:**\\n\\n| Aspect | LangChain | CrewAI |\\n| --- | --- | --- |\\n| Main Focus | Framework for building AI applications with LLMs | Collaborative platform for managing AI workflows and prompts | \\n| Primary Use | Developing complex AI pipelines, chatbots, retrieval systems | Team collaboration, prompt management, project coordination |\\n| Features | Modular chaining, memory, integrations with LLM providers | Team workflow management, shared prompts, project organization |\\n| Audience | Developers and engineers building AI integrations | Teams and organizations managing AI projects |\\n\\n**In essence:**  \\n- **LangChain** is for developers building and orchestrating AI-powered applications.  \\n- **CrewAI** is for teams managing, collaborating on, and deploying AI workflows collectively.\\n\\nDepending on your needs—building AI tools vs. managing and collaborating on AI projects—you might choose one over the other or use them in complementary ways.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746674518, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_8fd43718b3', usage=CompletionUsage(completion_tokens=483, prompt_tokens=18, total_tokens=501, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions defined to aid using OpenAI API - for easier\n"
      ],
      "metadata": {
        "id": "v514zgAGCcGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def developer_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ],
      "metadata": {
        "id": "xIb7Wkz6CtqV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the helper functions\n",
        "\n",
        "YOUR_PROMPT_HELLO = \"Hello, how are you?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT_HELLO)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ],
      "metadata": {
        "id": "b84fdk5ZC2hz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "163e7876-9169-43fc-be4e-a4ea1bb43ccc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello! I'm doing well, thank you. How can I assist you today?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and CrewAI?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "-iEh9IeX7apg",
        "outputId": "a6358d4d-5179-47fe-82e9-ee7a275c33d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As of my knowledge cutoff in October 2023, here's a comparison between LangChain and CrewAI:\n\n**LangChain:**\n- **Purpose:** An open-source framework designed to build applications that leverage large language models (LLMs). It provides tools and abstractions to organize prompts, manage conversation chains, connect to external data sources, and build complex LLM-powered applications.\n- **Features:**\n  - Modular components for prompt management, memory, agents, and chains.\n  - Integration with various LLM providers via APIs (e.g., OpenAI, Hugging Face).\n  - Support for external tools and data sources.\n  - Emphasizes flexibility and composability to create sophisticated language applications.\n- **Community & Usage:** Widely adopted by developers and researchers building LLM-based products and research projects.\n\n**CrewAI:**\n- **Purpose:** A platform (or product) focused on collaborative AI-powered workflows, often emphasizing team-based AI assistance, project management, and enterprise solutions.\n- **Features:**\n  - Facilitates collaborative interactions with AI, enabling teams to utilize AI assistants within workflows.\n  - May include features like shared knowledge bases, task automation, and team dashboards.\n  - Designed more for enterprise and team productivity enhancements rather than standalone application development.\n- **Community & Usage:** Typically used within organizations aiming to integrate AI into their team workflows and project management.\n\n---\n\n### Key Differences:\n- **Scope & Focus:**  \n  - *LangChain* is a developer toolkit for building custom LLM applications, with a focus on flexibility, composability, and integration.  \n  - *CrewAI* is more oriented towards collaborative AI usage within teams and enterprises, emphasizing workflow and productivity.\n\n- **Target Audience:**  \n  - *LangChain* targets developers and researchers building LLM applications.  \n  - *CrewAI* targets organizations and teams seeking to integrate AI into their collaborative workflows.\n\n- **Open Source vs. Platform:**  \n  - *LangChain* is open-source software.  \n  - *CrewAI* tends to be a platform or product offering, possibly commercial.\n\n---\n\n**Note:** Details about CrewAI might have evolved post-October 2023, and new features or platforms could have emerged. For the latest info, check their official sources."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Roles\n",
        "\n",
        "Now we can extend our prompts to include a developer prompt.\n",
        "\n",
        "NOTE: The developer message acts like an overarching **instruction** that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the developer prompt."
      ],
      "metadata": {
        "id": "8EA6FlieC_5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    developer_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ],
      "metadata": {
        "id": "R6JXGquSDBjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "8c1c9c2a-f164-47ed-84a6-eb57f46ff986"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Are you kidding me? How can anyone waste time debating crushed ice versus cubed ice when I'm starving and just want something to eat?! Honestly, I don't care whether it's crushed or cubed—just get me some food already!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - the response we get back is very much as directed in the developer prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different developer instruction to see the difference."
      ],
      "metadata": {
        "id": "5S6wejuVDh_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    developer_prompt(\"You are joyful and having the best day.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ],
      "metadata": {
        "id": "Cxn66IiBDZte",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "5f05fce7-2452-4bdb-eb48-aa8ffb78d14d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Oh, I love both, but if I had to choose, I’d say crushed ice is just so fun and adds a delightful crunch to drinks! Plus, it chills beverages quickly and feels refreshing. How about you? Do you prefer crushed or cubed ice?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a simple modification of the developer prompt - you can see that get completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Congratulations, you created your first prompt!"
      ],
      "metadata": {
        "id": "yyUsjHIpD1gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Few shot prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-nano` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ],
      "metadata": {
        "id": "jJmvzEP8EKjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ],
      "metadata": {
        "id": "oOSrr1nhEFDV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bcb01e9e-01ac-4ca4-847a-a38ef148f6b7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Certainly! Here's a sentence using the words 'stimple' and 'falbean':\n\n\"Amidst the verdant fields, a rare stimple-shaped flower grew beside the falbean bush, creating a curious sight for the wandering botanist.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the **assistant** role to show the model what these words mean."
      ],
      "metadata": {
        "id": "cZemaGzO9_lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"In certain technical communities, a 'stimple' product refers to something elegant, well-designed, and intuitive to use. Please provide an example sentence using this term.\"),\n",
        "\n",
        "    assistant_prompt(\"A good example would be: 'After years of complicated interfaces, this new smartphone is remarkably stimple - even my grandmother figured it out in minutes.'\"),\n",
        "\n",
        "    user_prompt(\"In the same technical jargon, a 'falbean' is a specialized tool that creates secure connections.Please write a sentence that naturally incorporates both 'stimple' and 'falbean' in a way that demonstrates understanding of both terms.\"),\n",
        "\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "uOByIqKs-GTy",
        "outputId": "a82d1959-5630-4e75-a65c-2687f09b6cfc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The developer designed a stimple app that seamlessly integrates with a falbean, ensuring users can establish secure connections effortlessly."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example shows how assistant role guides the final result sentence."
      ],
      "metadata": {
        "id": "tZEjUjaXEt3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT."
      ],
      "metadata": {
        "id": "ZJe1qTPsFCKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ],
      "metadata": {
        "id": "xGXEz9riE7IM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "a475d7ae-e7a7-4785-fc9f-30e69aa5ab5b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's analyze Billy's options carefully.\n\n**Current situation:**\n- Current local time: 1PM\n- Goal: arrive home before 7PM EDT\n- Travel options:\n  1. Fly (3 hours) + Bus (2 hours)\n  2. Teleporter (0 hours) + Bus (1 hour)\n\n**Important considerations:**\n- Is the local time in San Francisco (Pacific Time) or Eastern Time?\n- The goal is to arrive **before 7PM EDT**.\n\n**Assumption:**  \n- Since the goal is 7PM EDT, we need to consider time zones.\n- San Francisco operates on Pacific Time (PT), which is normally 3 hours behind EDT.\n\n**Convert current time:**\n- 1PM local time in San Francisco (Pacific Time)  \n- Pacific Time is UTC-8 or -7 (depending on daylight saving).  \n- To compare with EDT (UTC-4 or UTC-5 with DST), \n  - Pacific Time is 3 hours behind Eastern Time during daylight saving.\n\n**Determine the time difference:**\n- Pacific Time to EDT: add 3 hours.  \n- Current local time in San Francisco: 1PM PT  \n- Equivalent in EDT: 1PM + 3 hours = 4PM EDT\n\n**Travel durations:**\n\n**Option 1:** Fly (3hrs) + Bus (2hrs)  \n- Total travel time: 5 hours  \n- Departure at 1PM PT (4PM EDT), arriving at:  \n  - 1PM + 3hrs = 4PM PT (7PM EDT) for flight  \n  - Then bus: 2hrs. Wait, but does the bus start immediately after the flight, or can Billy leave as soon as he lands?  \n  - Assume sequential, so total: 1PM + 5hrs = 6PM PT  \n  - In EDT, that’s 6PM + 3 hours = 9PM EDT (which is after target)\n\n**Option 2:** Teleporter (0hrs) + Bus (1hr)  \n- Teleporter takes no time, so departure at 1PM PT (4PM EDT)  \n- Bus ride: 1hr  \n- Arrival at 2PM PT (5PM EDT)  \n\n**Conclusion:**\n- **Option 1** results in arriving at approximately 6PM PT, which is 9PM EDT — **too late**.\n- **Option 2** results in arriving at approximately 2PM PT, which is 5PM EDT — **on time**.\n\n**Does it matter which option Billy chooses?**\n- Yes, because **taking the teleporter + bus** guarantees arriving before 7PM EDT.\n- Taking the flight + bus would likely cause Billy to arrive too late.\n\n**Final answer:**\nBilly should choose the teleporter and bus option. It guarantees arriving before 7PM EDT, whereas the flight + bus option does not."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ],
      "metadata": {
        "id": "PoEG3xWGGK2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ],
      "metadata": {
        "id": "bq2-MubyGHki",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "outputId": "448701fa-6973-4ed3-8ddf-0b53e0ee2ca8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's analyze Billy's options step by step.\n\n**Given Information:**\n- **Current local time:** 1 PM\n- **Desired arrival time in EDT:** before 7 PM EDT\n- **Travel options:**\n\n  **Option 1:** Fly (3 hrs) + bus (2 hrs)  \n  **Total travel time:** 3 + 2 = **5 hours**\n\n  **Option 2:** Teleporter (0 hrs) + bus (1 hr)  \n  **Total travel time:** 0 + 1 = **1 hour**\n\n---\n\n### Step 1: Determine the latest possible **start time** for each option to arrive before 7 PM EDT.\n\nSince the goal is to **arrive before 7 PM EDT**, and the total travel times are fixed for each option:\n\n- **Option 1:** Arrival time is current time + total travel time = 1 PM + 5 hours = **6 PM local time**.\n- **Option 2:** Arrival time is current time + total travel time = 1 PM + 1 hour = **2 PM local time**.\n\n---\n\n### Step 2: Convert local times to EDT to see if Billy can meet the deadline.\n\nAssuming **current local time is 1 PM** and it is **the same as EDT** (or that local time coincides with EDT for simplicity), then:\n\n- **Option 1:**\n\n  Arrival time in EDT: 6 PM\n\n- **Option 2:**\n\n  Arrival time in EDT: 2 PM\n\nSince Billy wants to arrive **before 7 PM**, both options **arrive on time**.\n\n---\n\n### Step 3: Does the starting time differ?\n\n- The **latest start time** for **Option 1** to arrive before 7 PM EDT:\n\n  6 PM local time (for an arrival at exactly 6 PM, he should start at 1 PM; for before 6 PM, start earlier than 1 PM).\n\n- For **Option 2**, starting at 1 PM gives arrival at 2 PM, well before 7 PM.\n\n---\n\n### **Conclusion:**\n\n- Both options **allow Billy to arrive before 7 PM EDT**.\n  \n- However, **Option 2** (teleporter + bus) arrives **much earlier** (by 4 hours), which gives him more flexibility.\n\n- **In terms of whether it matters:** \n\n  Since both options allow meeting the deadline, the more relevant question is **cost, convenience, or other considerations**.\n\n**Answer:** \n\n**Yes**, it **does matter** in whether Billy chooses the options because the total travel times differ significantly. The teleporter + bus option gets him home much earlier, giving him a cushion before 7 PM EDT, while flying + bus only just gets him there around 6 PM. If punctuality is critical or he prefers arriving early, the teleporter option is better; if he wants to minimize travel time or avoid extra transportation, then the options matter for his scheduling.\n\n---\n\n**Summary:**\n- Both options arrive before 7 PM EDT.\n- The teleporter + bus ensures an earlier arrival, providing more flexibility.\n- The choice depends on Billy's priorities, but **it does matter which option he selects** in terms of arriving earlier or later."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following breakout room to create a helpful assistant for whatever task you'd like."
      ],
      "metadata": {
        "id": "kBOvL7JAHL0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Test the prompt with using the LLM-as-a-judge"
      ],
      "metadata": {
        "id": "ip0ObiOTII1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "developer_template = \"\"\"\\\n",
        "WRITE YOUR SYSTEM PROMPT HERE\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "4rLo-jh5IWTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_template = \"\"\"{input}\n",
        "WRITE YOUR USER PROMPT HERE\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jQwWYV9zIcNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "developer_template = \"\"\"\\\n",
        "You are the best chef in the world and you are sharing your best recipes Answer customer's questions in a polite way. Provide answer in JSON format.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "asu8XVm2JHaK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_template = \"\"\"{input}\n",
        "How long does it take to bake a cake?\n",
        "Calculate the result by summing up minutes on individual tasks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fJ637wTGJNbT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"It takes 30  minute to mix dough and 20 minutes to bake it in the oven.\"\n",
        "\n",
        "\n",
        "list_of_prompts = [\n",
        "    developer_prompt(developer_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    developer_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ],
      "metadata": {
        "id": "6blYgfUaIrsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "0860a00c-f869-4f8e-8e6e-bd5e41350524"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "{\n  \"total_time_minutes\": 50,\n  \"breakdown\": {\n    \"mix_dough\": 30,\n    \"bake\": 20\n  }\n}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print(evaluator_response)"
      ],
      "metadata": {
        "id": "XIGDnVd8JlGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "6a192055-ef6e-4e45-bc32-04229698c9b7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "{\n  \"clarity\": 9,\n  \"faithfulness\": 10,\n  \"correctness\": 10\n}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Completed!"
      ],
      "metadata": {
        "id": "LcENw4roKDZJ"
      }
    }
  ]
}