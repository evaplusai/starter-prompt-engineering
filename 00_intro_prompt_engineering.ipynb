{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Starter Prompt Engineering\n",
        "\n",
        "1. Getting started - setup env variables\n",
        "2. Using OpenAI library - client, roles\n",
        "3. Prompting examples - few shot, chain of thought\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note: How to get OpenAI key:\n",
        "\n",
        "Create an account with OpenAI [here](https://platform.openai.com/signup) if you do not have one.\n",
        "\n",
        "Click on the \"Settings\" icon at the top right,  then on the left menu navigate to \"API keys\". Click on \" Create new secret key\" and complete the screen. Make sure you Copy the key( you can always generate new one).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S4YFYTyPs5cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "First load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ],
      "metadata": {
        "id": "BD72fqZA72CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dependencies OpenAI library\n",
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "cO2w4bs78Lx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Environment Variables"
      ],
      "metadata": {
        "id": "eakszIMU8SED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import openai"
      ],
      "metadata": {
        "id": "nhxAWZTj5XB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
      ],
      "metadata": {
        "id": "Iu9b-aF08dUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. First Prompt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kWDnM32m8vy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use the ChatCompletion create method to interact with the \"gpt-4.1-nano\" model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "\n",
        "\n",
        "*   developer\n",
        "*   assistant\n",
        "*   user\n",
        "\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n"
      ],
      "metadata": {
        "id": "YL0HCv88Ay7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll explore these roles in more depth - but for now just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Again we will use latest model\n",
        "\n",
        "We'll use the `gpt-4.1` or `gpt-4.1-nano` model as stated above.\n",
        "\n",
        "Let's look at an example!\n"
      ],
      "metadata": {
        "id": "6elE8d_MBDRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models.\n",
        "\n",
        "> NOTE: You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) whenever you get stuck, have questions, or want to dive deeper."
      ],
      "metadata": {
        "id": "JWobf88a6UxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "wH0xIjtY6dum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_PROMPT = \"WRITE YOUR PROMPT HERE\"\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ],
      "metadata": {
        "id": "BrBeC-jZ5x40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions defined to aid using OpenAI API - for easier\n"
      ],
      "metadata": {
        "id": "v514zgAGCcGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def developer_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ],
      "metadata": {
        "id": "xIb7Wkz6CtqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the helper functions\n",
        "\n",
        "YOUR_PROMPT_HELLO = \"WRITE YOUR PROMPT HERE\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT_HELLO)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ],
      "metadata": {
        "id": "b84fdk5ZC2hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Roles\n",
        "\n",
        "Now we can extend our prompts to include a developer prompt.\n",
        "\n",
        "NOTE: The developer message acts like an overarching **instruction** that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the developer prompt."
      ],
      "metadata": {
        "id": "8EA6FlieC_5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    developer_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ],
      "metadata": {
        "id": "R6JXGquSDBjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - the response we get back is very much as directed in the developer prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different developer instruction to see the difference."
      ],
      "metadata": {
        "id": "5S6wejuVDh_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    developer_prompt(\"You are joyful and having the best day.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ],
      "metadata": {
        "id": "Cxn66IiBDZte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a simple modification of the developer prompt - you can see that get completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Congratulations, you created your first prompt!"
      ],
      "metadata": {
        "id": "yyUsjHIpD1gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Few shot prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-nano` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ],
      "metadata": {
        "id": "jJmvzEP8EKjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ],
      "metadata": {
        "id": "oOSrr1nhEFDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the **assistant** role to show the model what these words mean."
      ],
      "metadata": {
        "id": "cZemaGzO9_lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)\n",
        "\n"
      ],
      "metadata": {
        "id": "uOByIqKs-GTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example shows how assistant role guides the final result sentence."
      ],
      "metadata": {
        "id": "tZEjUjaXEt3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT."
      ],
      "metadata": {
        "id": "ZJe1qTPsFCKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\\n\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ],
      "metadata": {
        "id": "xGXEz9riE7IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task,  include \"Let's think step by step\":"
      ],
      "metadata": {
        "id": "PoEG3xWGGK2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"WRITE YOUR CoT STEP PROMPT HERE\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ],
      "metadata": {
        "id": "bq2-MubyGHki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following exampke to create a helpful assistant for whatever task you'd like."
      ],
      "metadata": {
        "id": "kBOvL7JAHL0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Test the prompt with using the LLM-as-a-judge"
      ],
      "metadata": {
        "id": "ip0ObiOTII1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "developer_template = \"\"\"\\\n",
        "You are the best chef in the world and you are sharing your best recipes Answer customer's questions in a polite way. Provide answer in JSON format.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "4rLo-jh5IWTl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_template = \"\"\"{input}\n",
        "How long does it take to bake a cake? Calculate the result by summing up minutes on individual tasks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jQwWYV9zIcNb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"It takes 30 minute to mix dough and 20 minutes to bake it in the oven.\"\n",
        "\n",
        "\n",
        "list_of_prompts = [\n",
        "    developer_prompt(developer_template),\n",
        "    user_prompt(user_template.format(input=query))\n",
        "]\n",
        "\n",
        "test_response = get_response(client, list_of_prompts)\n",
        "\n",
        "pretty_print(test_response)\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response\n",
        "3. Correctness - was the response correct?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    developer_prompt(evaluator_system_template),\n",
        "    user_prompt(evaluation_template.format(\n",
        "        input=query,\n",
        "        response=test_response.choices[0].message.content\n",
        "    ))\n",
        "]\n",
        "\n",
        "evaluator_response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=list_of_prompts,\n",
        "    response_format={\"type\" : \"json_object\"}\n",
        ")"
      ],
      "metadata": {
        "id": "6blYgfUaIrsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print(evaluator_response)"
      ],
      "metadata": {
        "id": "XIGDnVd8JlGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Completed!"
      ],
      "metadata": {
        "id": "LcENw4roKDZJ"
      }
    }
  ]
}